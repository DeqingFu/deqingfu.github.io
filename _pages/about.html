---
permalink: /
title: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% include base_path %}

<div style="font-family: Arial, sans-serif; line-height: 1.6; color: #333;">
  <p>Iâ€™m a third-year Ph.D. student in Computer Science at the <span style="color:#990000;">University of Southern California (USC)</span>. My main research interests are deep learning theory, natural language processing, and the interpretability of AI systems. I'm (co-)advised by <a href="https://vatsalsharan.github.io" style="color: #990000;">Prof. Vatsal Sharan</a> of <a href="https://viterbi-web.usc.edu/~cstheory/" style="color: #990000;">USC Theory Group</a> and <a href="https://robinjia.github.io" style="color: #990000;">Prof. Robin Jia</a> of <a href="https://allegro-lab.github.io" style="color: #990000;">Allegro Lab</a> within <a href="https://nlp.usc.edu" style="color: #990000;">USC NLP Group</a>, and I'm working closely with <a href="https://viterbi-web.usc.edu/~soltanol/index.html" style="color: #990000;">Prof. Mahdi Soltanolkotabi</a> and <a href="https://viterbi-web.usc.edu/~shanghua/" style="color: #990000;">Prof. Shang-Hua Teng</a>.</p>

  <h2 style="color: #990000; border-bottom: 2px solid #990000; padding-bottom: 5px;">Research</h2>
  <p>You can find a full picture of my research on the <a href="/publications/" style="color: #990000;">publications</a> page or my <a href="https://scholar.google.com/citations?user=fsbgfqEAAAAJ&hl=en" style="color: #990000;">Google Scholar</a> page. They belong to the following categories:</p>

  <h3 style="color: #990000; text-decoration: underline;">Algorithmic Perspectives on Large Language Models</h3>
  <ul>
    <li>How to think about LLMs and transformers architectures from a theoretical and algorithmic perspective?</li>
    <li>How transformer implements its capability of in-context learning? Is it really doing gradient descent in-context? <a href="https://arxiv.org/abs/2310.17086" style="color: #990000;">(NeurIPS 2024)</a></li>
    <li>How pretrained LLMs compute simple arithmetic tasks? Memorization or Mechanisms? <a href="https://arxiv.org/abs/2406.03445" style="color: #990000;">(NeurIPS 2024)</a></li>
    <li>Can we use classical decision theory to guide LLMs to make decisions under uncertainty? <a href="https://DeLLMa.github.io" style="color: #990000;">(ICLR 2025)</a></li>
  </ul>

  <h3 style="color: #990000; text-decoration: underline;">Synthetic Data and Multimodal Learning</h3>
  <ul>
    <li>How can we evaluate Multimodal LLMs/VLMs in a robust way? How could we improve them with synthetic data?</li>
    <li>Are Multimodal LLMs sensitive to the input modality of the same problem? <a href="https://arxiv.org/abs/2404.01266" style="color: #990000;">(COLM 2024)</a></li>
    <li>Improving Text-to-Image models with VLM's feedback. <a href="https://arxiv.org/abs/2311.17946" style="color: #990000;">(NAACL 2025)</a></li>
    <li>Evaluating Multimodal LLMs' hallucination rates by training a token-level reward model (TLDR). Improving Multimodal LLMs with TLDR at both training and inference time. <a href="https://arxiv.org/abs/2410.04734" style="color: #990000;">(ICLR 2025)</a></li>
  </ul>

  <h2 style="color: #990000; border-bottom: 2px solid #990000; padding-bottom: 5px;">Education</h2>
  <ul>
    <li>University of Southern California, 2022 - Present
      <ul>
        <li>Ph.D. in Computer Science</li>
      </ul>
    </li>
    <li>University of Chicago, 2016 - 2022
      <ul>
        <li>M.S. in Statistics</li>
        <li>B.S. <em>with Honors</em> in Mathematics, Computer Science, and Statistics</li>
      </ul>
    </li>
  </ul>
</div>
