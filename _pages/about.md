---
layout: about
title: Home
permalink: /
subtitle:

profile:
  align: right
  image: profile.jpg
  image_circular: false # crops the image to make it circular
  more_info: # blank

news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: false # includes social icons at the bottom of the page
---
This is **Deqing Fu** and I'm a fourth-year Ph.D. candidate in Computer Science at the <span style="color:#990000;"><strong class="fraktur-hover" data-text="University of Southern California">University of Southern California</strong>  (USC)</span>. My main research interests are **deep learning theory**, **natural language processing**, and the **interpretability of AI systems**. I'm (co-)advised by [Prof. Vatsal Sharan](https://vatsalsharan.github.io) of [USC Theory Group](https://viterbi-web.usc.edu/~cstheory/) and [Prof. Robin Jia](https://robinjia.github.io) of [Allegro Lab](https://allegro-lab.github.io) within [USC NLP Group](https://nlp.usc.edu), and I'm working closely with [Prof. Mahdi Soltanolkotabi](https://viterbi-web.usc.edu/~soltanol/index.html) and [Prof. Shang-Hua Teng](https://viterbi-web.usc.edu/~shanghua/). Before USC, I completed my undergraduate degree in Mathematics (with honors) and my masterâ€™s in Statistics at the <span style="color:#800000;"><strong class="fraktur-hover" data-text="University of Chicago">University of Chicago</strong></span>.

My research focuses on understanding large language models from algorithmic and theoretical perspectives, as well as developing methods for multimodal learning and synthetic data generation. You can find my publications on [Google Scholar](https://scholar.google.com/citations?user=fsbgfqEAAAAJ&hl=en) and my recent CV [here](/assets/pdf/curriculum_vitae_deqing.pdf).

###### **Algorithmic Perspectives on Large Language Models**
- Can Transformers learn algorithms simply from data? ([NeurIPS 2024](https://arxiv.org/abs/2310.17086), [ArXiv](https://arxiv.org/abs/2510.19753))
- Arithmetic in pretrained LLMs: memorization vs. mechanisms? ([NeurIPS 2024](https://arxiv.org/abs/2406.03445), [arXiv 2025](https://fouriernumber.github.io))
- What distinguishes Transformers from other architectures? ([ICLR 2025](https://arxiv.org/abs/2403.06925))
- Decision theory for LLM reasoning under uncertainty ([ICLR 2025 Spotlight](https://DeLLMa.github.io), [ArXiv(https://arxiv.org/abs/2601.07767)])

###### **Synthetic Data and Multimodal Learning**
- Modality sensitivity in Multimodal LLMs ([COLM 2024](https://arxiv.org/abs/2404.01266))
- VLM feedback for Text-to-Image generation ([NAACL 2025](https://arxiv.org/abs/2311.17946))
- Token-level reward models (TLDR) for reducing hallucinations ([ICLR 2025](https://arxiv.org/abs/2410.04734))