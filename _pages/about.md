---
permalink: /
title: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Iâ€™m a third-year Ph.D. student in Computer Science at the <span style="color:#990000;">University of Southern California (USC)</span>. My main research interests are deep learning theory, natural language processing, and the interpretability of AI systems. I'm (co-)advised by [Prof. Vatsal Sharan](https://vatsalsharan.github.io) of [USC Theory Group](https://viterbi-web.usc.edu/~cstheory/) and [Prof. Robin Jia](https://robinjia.github.io) of [Allegro Lab](https://allegro-lab.github.io) within [USC NLP Group](https://nlp.usc.edu), and I'm working closely with [Prof. Mahdi Soltanolkotabi](https://viterbi-web.usc.edu/~soltanol/index.html) and [Prof. Shang-Hua Teng](https://viterbi-web.usc.edu/~shanghua/).


## Research
You can find a full picture of my research in my [publication](/_pages/publications.html) page or my [Google scholar](https://scholar.google.com/citations?user=fsbgfqEAAAAJ&hl=en) page. They belong to the following categories.
<h3 style="text-decoration:underline">Algorithmic Perspectives on Large Language Models</h3>
How to think about LLMs and transformers architectures from a theoretical and algorithmic perspective? 
- How transformer implements its capabilitie of in-context learning? Is it really doing gradient descent in-context? [(NeurIPS 2024)](https://arxiv.org/abs/2310.17086).
- How pretrained LLMs compute simple arithmetic tasks? Memorization or Mechanisms? [(NeurIPS 2024)](https://arxiv.org/abs/2406.03445)

<h3 style="text-decoration:underline">Synthetic Data and Multimodal Learning</h3>

## Education
- University of Southern California, 2022 - Present
  - Ph.D. in Computer Science
- University of Chicago, 2016 - 2022
  - M.S. in Statistics
  - B.S. *with Honors* in Mathematics, Computer Science, and Statistics
