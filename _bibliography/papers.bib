---
---

%%%%%%%%%%%%%%%%%%%%%%%%%%%%ALGORITHMIC PERSPECTIVES ON LLMS%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{fu2024transformers,
  title={Transformers Learn to Achieve Second-Order Convergence Rates for In-Context Linear Regression},
  author={Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2024},
  arxiv={2310.17086},
  url={https://arxiv.org/abs/2310.17086},
  code={https://github.com/DeqingFu/transformers-icl-second-order},
  abstract={Transformers have shown remarkable in-context learning (ICL) capabilities. In this work, we study the ability of transformers to perform ICL for linear regression. We show that transformers trained on linear regression tasks with diverse covariates learn algorithms that achieve second-order convergence rates, outperforming first-order methods like gradient descent. Our theoretical analysis and experiments demonstrate that transformers can learn to implement algorithms similar to Newton's method and preconditioned gradient descent.},
  selected={true},
  category={conference},
  note={<span style="color: orange;">SoCalNLP Symposium 2023 Best Paper Award</span>},
  preview={transformer-icl.png}
}

@inproceedings{zhou2024fourier,
  title={Pre-trained Large Language Models Use Fourier Features to Compute Addition},
  author={Zhou, Tianyi and Fu, Deqing and Sharan, Vatsal and Jia, Robin},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2024},
  arxiv={2406.03445},
  url={https://arxiv.org/abs/2406.03445},
  abstract={We study how pre-trained large language models compute arithmetic tasks. We find that these models use Fourier features to represent and manipulate numbers, providing insights into their computational mechanisms.},
  selected={true},
  category={conference},
  preview={fourier.png}
}

@inproceedings{zhou2025fone,
  title={FoNE: Precise Single-Token Number Embeddings via Fourier Features},
  author={Zhou, Tianyi and Fu, Deqing and Soltanolkotabi, Mahdi and Jia, Robin and Sharan, Vatsal},
  booktitle={arXiv},
  year={2025},
  arxiv={2502.09741},
  url={https://arxiv.org/abs/2502.09741},
  website={https://fouriernumber.github.io},
  abstract={We propose FoNE, a method for encoding numbers as single tokens using Fourier features, enabling more precise numerical reasoning in language models.},
  selected={false},
  category={preprint},
  preview={fone.png}
}

@inproceedings{vasudeva2025sensitivity,
  title={Transformers Learn Low Sensitivity Functions: Investigations and Implications},
  author={Vasudeva*, Bhavya and Fu*, Deqing and Zhou, Tianyi and Kau, Elliot and Huang, You-Qi and Sharan, Vatsal},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025},
  arxiv={2403.06925},
  url={https://arxiv.org/abs/2403.06925},
  abstract={We investigate the types of functions that transformers learn and find that they preferentially learn low sensitivity functions. We explore the implications of this finding for understanding transformer capabilities and limitations.},
  selected={true},
  category={conference},
  note={*Equal Contribution},
  preview={sensitivity.png}
}

@inproceedings{liu2025dellma,
  title={DeLLMa: Decision Making Under Uncertainty with Large Language Models},
  author={Liu*, Ollie and Fu*, Deqing and Yogatama, Dani and Neiswanger, Willie},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025},
  arxiv={2402.02392},
  url={https://arxiv.org/abs/2402.02392},
  code={https://github.com/DeLLMa/DeLLMa},
  website={https://dellma.github.io/},
  abstract={We introduce DeLLMa, a framework for using large language models to make decisions under uncertainty by combining classical decision theory with modern language models.},
  selected={true},
  category={conference},
  note={<span style="color: orange;">Spotlight (Top 5.1%)</span>, *Equal Contribution},
  preview={dellma.png}
}

@inproceedings{gan2025steering,
  title={Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models},
  author={Gan*, Woody Haosheng and Fu*, Deqing and Asilis*, Julian and Liu*, Ollie and Yogatama, Dani and Sharan, Vatsal and Jia, Robin and Neiswanger, Willie},
  booktitle={arXiv},
  year={2025},
  arxiv={2505.14071},
  url={https://arxiv.org/abs/2505.14071},
  blog={https://www.lesswrong.com/posts/XwxSYoFWijSwrHF6n/untitled-draft-nr3m?utm_campaign=post_share&utm_source=link},
  abstract={We show that textual steering vectors can improve visual understanding in multimodal large language models, bridging the gap between language and vision modalities.},
  selected={true},
  category={preprint},
  note={*Equal Contribution},
  preview={saegull.png}
}

@inproceedings{wang2025resa,
  title={Resa: Transparent Reasoning Models via SAEs},
  author={Wang, Shangshang and Asilis, Julian and Akgül, Ömer Faruk and Bilgin, Enes Burak and Liu, Ollie and Fu, Deqing and Neiswanger, Willie},
  booktitle={arXiv},
  year={2025},
  arxiv={2506.09967},
  url={https://arxiv.org/abs/2506.09967},
  abstract={We introduce Resa, a method for creating transparent reasoning models using Sparse Autoencoders (SAEs) to better understand and control reasoning processes in language models.},
  selected={false},
  category={preprint},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%SYNTHETIC DATA AND MULTIMODAL LEARNING%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{li2025zebracot,
  title={Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning},
  author={Li*, Ang and Wang*, Charles and Yue*, Kaiyu and Cai*, Zikui and Liu*, Ollie and Fu*, Deqing and Guo*, Peng and Zhu*, Wang Bill and Sharan, Vatsal and Jia, Robin and Neiswanger, Willie and Huang, Furong and Goldstein, Tom and Goldblum, Micah},
  booktitle={arXiv},
  year={2025},
  arxiv={2507.16746},
  url={https://arxiv.org/abs/2507.16746},
  huggingface_datasets={https://huggingface.co/datasets/multimodal-reasoning-lab/Zebra-CoT},
  abstract={We introduce Zebra-CoT, a diverse large-scale dataset with 182,384 samples containing logically coherent interleaved text-image reasoning traces for training multimodal models to use visual aids when solving complex problems.},
  selected={true},
  category={preprint},
  note={*Equal Contribution},
  preview={zebra-cot.png}
}

@inproceedings{zhu2024visuallens,
  title={VisualLens: Personalization through Visual History},
  author={Zhu, Wang Bill and Fu, Deqing and Sun, Kai and Lu, Yi and Lin, Zhaojiang and Moon, Seungwhan and Narang, Kanika and Canim, Mustafa and Liu, Yue and Kumar, Anuj and Dong, Xin Luna},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2025},
  arxiv={2411.16034},
  url={https://arxiv.org/abs/2411.16034},
  abstract={We introduce VisualLens, a system for personalization through visual history that enables more contextual and personalized AI interactions.},
  selected={false},
  category={preprint},
}

@inproceedings{fu2025tldr,
  title={TLDR: Token-Level Detective Reward Model for Large Vision Language Models},
  author={Fu, Deqing and Xiao, Tong and Wang, Rui and Zhu, Wang and Zhang, Pengchuan and Pang, Guan and Jia, Robin and Chen, Lawrence},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025},
  arxiv={2410.04734},
  url={https://arxiv.org/abs/2410.04734},
  abstract={We propose TLDR, a token-level detective reward model for evaluating and improving large vision language models by identifying hallucinations at the token level.},
  selected={true},
  category={conference},
  preview={tldr.png}
}

@inproceedings{fu2024isobench,
  title={IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations},
  author={Fu*, Deqing and Guo*, Ruohao and Khalighinejad*, Ghazal and Liu*, Ollie and Dhingra, Bhuwan and Yogatama, Dani and Jia, Robin and Neiswanger, Willie},
  booktitle={Conference on Language Modeling (COLM)},
  year={2024},
  arxiv={2404.01266},
  url={https://arxiv.org/abs/2404.01266},
  website={https://isobench.github.io},
  abstract={We introduce IsoBench, a benchmark for evaluating whether multimodal models are sensitive to the input modality for semantically equivalent problems presented in different formats.},
  selected={true},
  category={conference},
  note={*Equal Contribution},
  preview={isobench.png}
}

@inproceedings{sun2025dreamsync,
  title={DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback},
  author={Sun*, Jiao and Fu*, Deqing and Hu*, Yushi and Wang, Su and Rassin, Royi and Juan, Da-Cheng and Alon, Dana and Herrmann, Charles and van Steenkiste, Sjoerd and Krishna, Ranjay and Rashtchian, Cyrus},
  booktitle={Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2025},
  arxiv={2311.17946},
  url={https://arxiv.org/abs/2311.17946},
  abstract={We propose DreamSync, a method for improving text-to-image generation by using feedback from vision-language models to align generated images with text prompts.},
  selected={true},
  category={conference},
  note={*Equal Contribution},
  preview={dreamsync.png}
}

@inproceedings{fu2023scene,
  title={SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative Examples},
  author={Fu, Deqing and Godbole, Ameya and Jia, Robin},
  booktitle={Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
  arxiv={2305.07984},
  url={https://arxiv.org/abs/2305.07984},
  code={https://github.com/DeqingFu/scene},
  abstract={We propose SCENE, a method for generating self-labeled counterfactual examples to help models extrapolate to negative examples and improve robustness.},
  selected={false},
  category={conference},
  preview={scene.png}
}
